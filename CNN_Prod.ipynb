{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Set execution role\n",
    "role = get_execution_role()\n",
    "bucket ='sagemaker-nomadiq-data'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.Bucket('sagemaker-nomadiq-data').download_file('vocab_labels.pickle','vocab_labels.pickle')\n",
    "s3.Bucket('sagemaker-nomadiq-data').download_file('testnet.params','testnet.params')\n",
    "\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading saved parameters for forward pass\n",
    "We redefine our architecture and load parameters with Gluon's load_parameters method. Now we're ready to make predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r'vocab_labels.pickle', 'rb') as f:\n",
    "    vocabulary2,vocab_size2,sentence_size2,labelencoder2,unique_labels2,test_accuracy_list2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads and preprocessed data for the dataset. Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
      "data shape: (1, 461)\n"
     ]
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"<b>\", \" \", string)\n",
    "    string = re.sub(r\"</b>\", \" \", string)\n",
    "    string = re.sub(r\"<br>\", \" \", string)\n",
    "    string = re.sub(r\"</br>\", \" \", string)\n",
    "    string = re.sub(r\"<p>\", \" \", string)\n",
    "    string = re.sub(r\"</p>\", \" \", string)\n",
    "    string = re.sub(r\"<ul>\", \" \", string)\n",
    "    string = re.sub(r\"</ul>\", \" \", string)\n",
    "    string = re.sub(r\"<li>\", \" \", string)\n",
    "    string = re.sub(r\"</li>\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def load_data_and_labels(string):\n",
    "    \"\"\"\n",
    "    Loads and cleans training data\n",
    "    \"\"\"\n",
    "    # Get raw data and create concatenated text string\n",
    "    x_text = string\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    return x_text\n",
    "\n",
    "def pad_sentences_oos(sentences, padding_word=\"</s>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sentence_size2 - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "def fit_input_data(sentences, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    list_1 = []\n",
    "    for sentence in sentences:\n",
    "        list_2 = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                list_2.append(vocabulary2[word])\n",
    "            except:\n",
    "                list_2.append(0)\n",
    "        list_1.append(list_2)\n",
    "    x = np.array(list_1)\n",
    "    return x\n",
    "\n",
    "print(\"Loads and preprocessed data for the dataset. Returns input vectors, labels, vocabulary, and inverse vocabulary.\")\n",
    "# Load and preprocess data\n",
    "# input_sent = ['surfing beach is sunny and beautiful today']\n",
    "# sentences = load_data_and_labels(input_sent)\n",
    "sentences_padded = pad_sentences_oos(sentences)\n",
    "# vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "x = fit_input_data(sentences_padded, vocabulary2)\n",
    "\n",
    "print('data shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get recommendations based on string input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What type of vacation are you interested in? (Free-form entry):  fishing \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads and preprocessed data for the dataset. Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
      "data shape: (1, 461)\n",
      "Sequential(\n",
      "  (0): Embedding(100710 -> 300, float32)\n",
      "  (1): Conv1D(None -> 50, kernel_size=(3,), stride=(1,))\n",
      "  (2): MaxPool1D(size=(2,), stride=(2,), padding=(0,), ceil_mode=False)\n",
      "  (3): Dropout(p = 0.5, axes=())\n",
      "  (4): Flatten\n",
      "  (5): Dense(None -> 256, Activation(relu))\n",
      "  (6): Dropout(p = 0.5, axes=())\n",
      "  (7): Dense(None -> 200, linear)\n",
      ")\n",
      "Here is your recommended destination: \n",
      "0: Bangkok, Thailand\n",
      "1: Havana, Cuba\n",
      "2: London, United Kingdom\n",
      "3: Los Angeles, California\n",
      "4: Thailand\n"
     ]
    }
   ],
   "source": [
    "text_string = []\n",
    "text_string.append(input(\"What type of vacation are you interested in? (Free-form entry): \"))\n",
    "\n",
    "# Take input and make prediction\n",
    "\n",
    "print(\"Loads and preprocessed data for the dataset. Returns input vectors, labels, vocabulary, and inverse vocabulary.\")\n",
    "# Load and preprocess data\n",
    "sentences = load_data_and_labels(text_string)\n",
    "sentences_padded = pad_sentences_oos(sentences)\n",
    "# vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "x = fit_input_data(sentences_padded, vocabulary2)\n",
    "\n",
    "print('data shape:', x.shape)\n",
    "\n",
    "batch_size = 50\n",
    "vocab_size = vocab_size2\n",
    "embed_size = 300\n",
    "num_fc = 256\n",
    "num_filters = 50\n",
    "filter_size = 3\n",
    "unique_labels = unique_labels2\n",
    "\n",
    "# New input data to predict on\n",
    "prod_data = gluon.data.DataLoader(gluon.data.ArrayDataset(x),\n",
    "                                      batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Net2 is a copy of the original architecture\n",
    "net2 = gluon.nn.Sequential()\n",
    "with net2.name_scope():\n",
    "    net2.add(gluon.nn.Embedding(vocab_size, embed_size))\n",
    "    net2.add(gluon.nn.Conv1D(channels = num_filters, kernel_size = filter_size, activation='relu'))\n",
    "    net2.add(gluon.nn.MaxPool1D(pool_size=2,strides=2))\n",
    "    net2.add(gluon.nn.Dropout(.5))\n",
    "    net2.add(gluon.nn.Flatten())\n",
    "    net2.add(gluon.nn.Dense(num_fc, activation = 'relu'))\n",
    "    net2.add(gluon.nn.Dropout(.5))\n",
    "    net2.add(gluon.nn.Dense(unique_labels))\n",
    "\n",
    "\n",
    "print(net2)\n",
    "# Loading the previously saved training parameters \n",
    "net2.load_parameters(r'testnet.params', ctx=ctx)\n",
    "\n",
    "# Forward pass that results in a list of ndarrays with predicted ITK and softmax probabilities associated with the predicted ITK.  \n",
    "prediction_list = []\n",
    "probabilities = []\n",
    "for i, data in enumerate(prod_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        output = net2(data).softmax()\n",
    "        max_prob = np.amax(output, axis=1)\n",
    "#         predictions = nd.argmax(output, axis=1)\n",
    "        prediction_list =  nd.argsort(output,axis = 1)[0][::-1][:5]\n",
    "#         prediction_list.append(predictions)\n",
    "        probabilities.append(max_prob)\n",
    "        \n",
    "# Transform to numpy arrays in int and float dtypes (vs. ndarray) to apply to input datafram\n",
    "pred2 = [pred.astype(\"int\").asnumpy() for pred in prediction_list]\n",
    "transformed_prediction = labelencoder2.inverse_transform(pred2)\n",
    "prob2 = [prob.astype(\"float32\").asnumpy() for prob in probabilities]\n",
    "\n",
    "flat_predictions = [item for sublist in transformed_prediction for item in sublist]\n",
    "flat_prob = [item for sublist in prob2 for item in sublist]\n",
    "print(\"Here is your recommended destination: \")\n",
    "for i, x in enumerate(flat_predictions):\n",
    "    print(str(i)+\":\",x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[  54.   53.  134.   69.  178.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.argsort(output,axis = 1)[0][:5]\n",
    "\n",
    "# import numpy as np\n",
    "# x = np.arange(10)\n",
    "# print(\"Original array:\")\n",
    "# print(x)\n",
    "# np.random.shuffle(x)\n",
    "# n = 1\n",
    "# print (x[np.argsort(x)[-n:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Pandas dataframe that displays input data and predicted ITK (\"prediction\") and softmax probability of those predictions (\"probability\") \n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# df = pd.read_csv('/tmp/eider-user/userfile/'+username+'/itk_data_oos2.csv', encoding = 'latin-1')\n",
    "\n",
    "# # Run the entire dataset and save results to S3\n",
    "# df['prediction'] = flat_predictions\n",
    "# df['probability'] = flat_prob\n",
    "# df.to_csv('/tmp/eider-user/userfile/'+username+'/results.csv',encoding = 'latin-1')\n",
    "# s3.Bucket('itk-model-data').upload_file('/tmp/eider-user/userfile/'+username+'/results.csv', 'results.csv')\n",
    " \n",
    "# # Print a sample of predictions\n",
    "# df[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Balance out all data (by doubling it)\n",
    "- Tripadvisor data\n",
    "- Reddit (open API)\n",
    "- Yelp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
