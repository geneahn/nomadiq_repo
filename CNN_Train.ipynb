{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Set execution role\n",
    "role = get_execution_role()\n",
    "bucket='sagemaker-nomadiq-data'\n",
    "data_key = 'instagram_df.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "# ctx = mx.gpu()\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>location_id</th>\n",
       "      <th>location_name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>month</th>\n",
       "      <th>datetime_readable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hair kissed by the sun ‚òÄ\\n‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.548702e+09</td>\n",
       "      <td>['ad', 'playrescuereset', 'pantenerescueshots']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970-01-01 00:00:01.548701796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One of my favorite views in the world üó∫ ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢...</td>\n",
       "      <td>215871574.0</td>\n",
       "      <td>Cape Town, Western Cape</td>\n",
       "      <td>1.548620e+09</td>\n",
       "      <td>['mavic2pro']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970-01-01 00:00:01.548620093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Got my sights set on Sydney! üê®üá¶üá∫ ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢...</td>\n",
       "      <td>2112249.0</td>\n",
       "      <td>Sydney Opera House</td>\n",
       "      <td>1.548445e+09</td>\n",
       "      <td>['sydney', 'australia']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970-01-01 00:00:01.548445014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Planning a bucket list trip in 2019? #CapitalO...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.548181e+09</td>\n",
       "      <td>['capitalonepartner']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970-01-01 00:00:01.548180883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Back home in my favorite city ‚ô•Ô∏èüáøüá¶\\n‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢...</td>\n",
       "      <td>215871574.0</td>\n",
       "      <td>Cape Town, Western Cape</td>\n",
       "      <td>1.548102e+09</td>\n",
       "      <td>['capetown', 'southafrica', 'tbapresets']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970-01-01 00:00:01.548101892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caption  location_id  \\\n",
       "0  Hair kissed by the sun ‚òÄ\\n‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢...          NaN   \n",
       "1  One of my favorite views in the world üó∫ ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢...  215871574.0   \n",
       "2  Got my sights set on Sydney! üê®üá¶üá∫ ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢...    2112249.0   \n",
       "3  Planning a bucket list trip in 2019? #CapitalO...          NaN   \n",
       "4  Back home in my favorite city ‚ô•Ô∏èüáøüá¶\\n‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢...  215871574.0   \n",
       "\n",
       "             location_name     timestamp  \\\n",
       "0                      NaN  1.548702e+09   \n",
       "1  Cape Town, Western Cape  1.548620e+09   \n",
       "2       Sydney Opera House  1.548445e+09   \n",
       "3                      NaN  1.548181e+09   \n",
       "4  Cape Town, Western Cape  1.548102e+09   \n",
       "\n",
       "                                          hashtags  month  \\\n",
       "0  ['ad', 'playrescuereset', 'pantenerescueshots']    1.0   \n",
       "1                                    ['mavic2pro']    1.0   \n",
       "2                          ['sydney', 'australia']    1.0   \n",
       "3                            ['capitalonepartner']    1.0   \n",
       "4        ['capetown', 'southafrica', 'tbapresets']    1.0   \n",
       "\n",
       "               datetime_readable  \n",
       "0  1970-01-01 00:00:01.548701796  \n",
       "1  1970-01-01 00:00:01.548620093  \n",
       "2  1970-01-01 00:00:01.548445014  \n",
       "3  1970-01-01 00:00:01.548180883  \n",
       "4  1970-01-01 00:00:01.548101892  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "df = pd.read_csv(data_location)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads and preprocessed data for the given dataset. Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
      "Train/Dev split: 33360/8340\n",
      "train shape: (33360, 461)\n",
      "dev shape: (8340, 461)\n",
      "vocab_size 100710\n",
      "sentence max words 461\n",
      "unique labels 200\n"
     ]
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"<b>\", \" \", string)\n",
    "    string = re.sub(r\"</b>\", \" \", string)\n",
    "    string = re.sub(r\"<br>\", \" \", string)\n",
    "    string = re.sub(r\"</br>\", \" \", string)\n",
    "    string = re.sub(r\"<p>\", \" \", string)\n",
    "    string = re.sub(r\"</p>\", \" \", string)\n",
    "    string = re.sub(r\"<ul>\", \" \", string)\n",
    "    string = re.sub(r\"</ul>\", \" \", string)\n",
    "    string = re.sub(r\"<li>\", \" \", string)\n",
    "    string = re.sub(r\"</li>\", \" \", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def load_data_and_labels(datafile):\n",
    "    \"\"\"\n",
    "    Loads and cleans training data\n",
    "    \"\"\"\n",
    "    # Get raw data and create concatenated text string    \n",
    "    df = pd.read_csv(datafile,encoding = 'latin-1')\n",
    "    # Remove missing locations\n",
    "    df = df[df.location_name.notnull()]\n",
    "    df = df.replace(np.nan, '', regex=True)\n",
    "    # Get location names associated with the top 200 most popular locations\n",
    "    df = df[df['location_name'].isin(df.location_name.value_counts()[:200].index.values)]\n",
    "    x_text = df['caption']\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    y = df['location_name']\n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    labels = labelencoder.fit_transform(y)\n",
    "    return [x_text, labelencoder, labels]\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"</s>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "print(\"Loads and preprocessed data for the given dataset. Returns input vectors, labels, vocabulary, and inverse vocabulary.\")\n",
    "# Load and preprocess data\n",
    "sentences, labelencoder, labels = load_data_and_labels(data_location)\n",
    "sentences_padded = pad_sentences(sentences)\n",
    "vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# split train/dev set\n",
    "# there are a total of 10662 labeled examples to train on\n",
    "x_train, x_dev = x_shuffled[:round(len(x)*.8)], x_shuffled[round(len(x)*.8):]\n",
    "y_train, y_dev = y_shuffled[:round(len(x)*.8)], y_shuffled[round(len(x)*.8):]\n",
    "\n",
    "sentence_size = x_train.shape[1]\n",
    "unique_labels = len(np.unique(y))\n",
    "\n",
    "print('Train/Dev split: %d/%d' % (len(y_train), len(y_dev)))\n",
    "print('train shape:', x_train.shape)\n",
    "print('dev shape:', x_dev.shape)\n",
    "print('vocab_size', vocab_size)\n",
    "print('sentence max words', sentence_size)\n",
    "print('unique labels', unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Conv Net architecture\n",
    "This architecture borrows heavily from Yoon Kim's paper \"Convolutional Neural Networks for Sentence Classification\". <https://arxiv.org/pdf/1408.5882.pdf>\n",
    "\n",
    "In this paper, Kim found that CNNs can provide state-of-the-art results on sentence classifications across numerous contexts (e.g., sentiment, movie reviews, etc.). He used an embedding layer to squish words into lower-dimensional (300 dims) vector representations vs. a sparse one-hot encoded matrix (which would be nearly 100K dims depending on the size of our examples). He then had a single convolutional layer with multiple filter widths (3,4,and 5-grams) to detect \"features\", which then went through a max pool layer, pushed through a fully connected layer (256 nodes), and finally a softmax layer to get label predictions. We use dropout regularization to prevent overfitting. \n",
    "\n",
    "We load both test and training data using Gluon's out-of-the-box data loader and set batch size to 50. Also, we use Gluon's sequential class to define the architecture, as it easy to understand and simple to implement. \n",
    "\n",
    "Areas of improvement/tuning: \n",
    "- I could not figure out how to have multiple filter widths in the Gluon framework. Kim used (3,4,and 5), but I'm only using 3. To achieve, this may need some hybrid model with both vanilla mxnet and Gluon. \n",
    "- In this model, we are training our own word embeddings. My rationale is that pre-trained embeddings (e.g., GloVe, Word2vec) may not have all the instagram-specific strings like brands and models, but I could be wrong. We should try testing this model with existing pre-trained embeddings. In Kim's paper, there were better results using pre-trained embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Embedding(100710 -> 300, float32)\n",
      "  (1): Conv1D(None -> 50, kernel_size=(3,), stride=(1,))\n",
      "  (2): MaxPool1D(size=(2,), stride=(2,), padding=(0,), ceil_mode=False)\n",
      "  (3): Dropout(p = 0.5, axes=())\n",
      "  (4): Flatten\n",
      "  (5): Dense(None -> 256, Activation(relu))\n",
      "  (6): Dropout(p = 0.5, axes=())\n",
      "  (7): Dense(None -> 200, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "embed_size = 300\n",
    "# embed_size = 150\n",
    "num_fc = 256\n",
    "num_filters = 50\n",
    "filter_size = 3\n",
    "\n",
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(x_train, y_train),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.ArrayDataset(x_dev, y_dev),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Embedding(vocab_size, embed_size))\n",
    "    net.add(gluon.nn.Conv1D(channels = num_filters, kernel_size = filter_size, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=2,strides=2))\n",
    "    net.add(gluon.nn.Dropout(.5))\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(num_fc, activation = 'relu'))\n",
    "    net.add(gluon.nn.Dropout(.5))\n",
    "    net.add(gluon.nn.Dense(unique_labels))\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: UserWarning: All children of this Sequential layer 'sequential4_' are HybridBlocks. Consider using HybridSequential for the best performance.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: plot Pages: 1 -->\n",
       "<svg width=\"102pt\" height=\"1006pt\"\n",
       " viewBox=\"0.00 0.00 102.00 1006.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1002)\">\n",
       "<title>plot</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1002 98,-1002 98,4 -4,4\"/>\n",
       "<!-- data -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>data</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"#000000\" cx=\"47\" cy=\"-29\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-25.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">data</text>\n",
       "</g>\n",
       "<!-- sequential4_embedding0_fwd -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>sequential4_embedding0_fwd</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"94,-152 0,-152 0,-94 94,-94 94,-152\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">sequential4_embedding0_fwd</text>\n",
       "</g>\n",
       "<!-- sequential4_embedding0_fwd&#45;&gt;data -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>sequential4_embedding0_fwd&#45;&gt;data</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-83.6321C47,-75.1148 47,-66.2539 47,-58.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-93.7731 42.5001,-83.773 47,-88.7731 47.0001,-83.7731 47.0001,-83.7731 47.0001,-83.7731 47,-88.7731 51.5001,-83.7731 47,-93.7731 47,-93.7731\"/>\n",
       "</g>\n",
       "<!-- sequential4_conv0_fwd -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>sequential4_conv0_fwd</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"#000000\" points=\"94,-246 0,-246 0,-188 94,-188 94,-246\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Convolution</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-205.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3/1, 50</text>\n",
       "</g>\n",
       "<!-- sequential4_conv0_fwd&#45;&gt;sequential4_embedding0_fwd -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>sequential4_conv0_fwd&#45;&gt;sequential4_embedding0_fwd</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-177.6321C47,-169.1148 47,-160.2539 47,-152.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-187.7731 42.5001,-177.773 47,-182.7731 47.0001,-177.7731 47.0001,-177.7731 47.0001,-177.7731 47,-182.7731 51.5001,-177.7731 47,-187.7731 47,-187.7731\"/>\n",
       "</g>\n",
       "<!-- sequential4_conv0_relu_fwd -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>sequential4_conv0_relu_fwd</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"#000000\" points=\"94,-340 0,-340 0,-282 94,-282 94,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-299.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">relu</text>\n",
       "</g>\n",
       "<!-- sequential4_conv0_relu_fwd&#45;&gt;sequential4_conv0_fwd -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>sequential4_conv0_relu_fwd&#45;&gt;sequential4_conv0_fwd</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-271.6321C47,-263.1148 47,-254.2539 47,-246.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-281.7731 42.5001,-271.773 47,-276.7731 47.0001,-271.7731 47.0001,-271.7731 47.0001,-271.7731 47,-276.7731 51.5001,-271.7731 47,-281.7731 47,-281.7731\"/>\n",
       "</g>\n",
       "<!-- sequential4_pool0_fwd -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>sequential4_pool0_fwd</title>\n",
       "<polygon fill=\"#80b1d3\" stroke=\"#000000\" points=\"94,-434 0,-434 0,-376 94,-376 94,-434\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-408.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Pooling</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-393.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">max, 2/2</text>\n",
       "</g>\n",
       "<!-- sequential4_pool0_fwd&#45;&gt;sequential4_conv0_relu_fwd -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>sequential4_pool0_fwd&#45;&gt;sequential4_conv0_relu_fwd</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-365.6321C47,-357.1148 47,-348.2539 47,-340.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-375.7731 42.5001,-365.773 47,-370.7731 47.0001,-365.7731 47.0001,-365.7731 47.0001,-365.7731 47,-370.7731 51.5001,-365.7731 47,-375.7731 47,-375.7731\"/>\n",
       "</g>\n",
       "<!-- sequential4_dropout0_fwd -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>sequential4_dropout0_fwd</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"94,-528 0,-528 0,-470 94,-470 94,-528\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-495.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">sequential4_dropout0_fwd</text>\n",
       "</g>\n",
       "<!-- sequential4_dropout0_fwd&#45;&gt;sequential4_pool0_fwd -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>sequential4_dropout0_fwd&#45;&gt;sequential4_pool0_fwd</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-459.6321C47,-451.1148 47,-442.2539 47,-434.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-469.7731 42.5001,-459.773 47,-464.7731 47.0001,-459.7731 47.0001,-459.7731 47.0001,-459.7731 47,-464.7731 51.5001,-459.7731 47,-469.7731 47,-469.7731\"/>\n",
       "</g>\n",
       "<!-- sequential4_flatten0_flatten0 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>sequential4_flatten0_flatten0</title>\n",
       "<polygon fill=\"#fdb462\" stroke=\"#000000\" points=\"94,-622 0,-622 0,-564 94,-564 94,-622\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-589.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">sequential4_flatten0_flatten0</text>\n",
       "</g>\n",
       "<!-- sequential4_flatten0_flatten0&#45;&gt;sequential4_dropout0_fwd -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>sequential4_flatten0_flatten0&#45;&gt;sequential4_dropout0_fwd</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-553.6321C47,-545.1148 47,-536.2539 47,-528.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-563.7731 42.5001,-553.773 47,-558.7731 47.0001,-553.7731 47.0001,-553.7731 47.0001,-553.7731 47,-558.7731 51.5001,-553.7731 47,-563.7731 47,-563.7731\"/>\n",
       "</g>\n",
       "<!-- sequential4_dense0_fwd -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>sequential4_dense0_fwd</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"#000000\" points=\"94,-716 0,-716 0,-658 94,-658 94,-716\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-690.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-675.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">256</text>\n",
       "</g>\n",
       "<!-- sequential4_dense0_fwd&#45;&gt;sequential4_flatten0_flatten0 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>sequential4_dense0_fwd&#45;&gt;sequential4_flatten0_flatten0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-647.6321C47,-639.1148 47,-630.2539 47,-622.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-657.7731 42.5001,-647.773 47,-652.7731 47.0001,-647.7731 47.0001,-647.7731 47.0001,-647.7731 47,-652.7731 51.5001,-647.7731 47,-657.7731 47,-657.7731\"/>\n",
       "</g>\n",
       "<!-- sequential4_dense0_relu_fwd -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>sequential4_dense0_relu_fwd</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"#000000\" points=\"94,-810 0,-810 0,-752 94,-752 94,-810\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-784.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-769.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">relu</text>\n",
       "</g>\n",
       "<!-- sequential4_dense0_relu_fwd&#45;&gt;sequential4_dense0_fwd -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>sequential4_dense0_relu_fwd&#45;&gt;sequential4_dense0_fwd</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-741.6321C47,-733.1148 47,-724.2539 47,-716.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-751.7731 42.5001,-741.773 47,-746.7731 47.0001,-741.7731 47.0001,-741.7731 47.0001,-741.7731 47,-746.7731 51.5001,-741.7731 47,-751.7731 47,-751.7731\"/>\n",
       "</g>\n",
       "<!-- sequential4_dropout1_fwd -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>sequential4_dropout1_fwd</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"94,-904 0,-904 0,-846 94,-846 94,-904\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-871.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">sequential4_dropout1_fwd</text>\n",
       "</g>\n",
       "<!-- sequential4_dropout1_fwd&#45;&gt;sequential4_dense0_relu_fwd -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>sequential4_dropout1_fwd&#45;&gt;sequential4_dense0_relu_fwd</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-835.6321C47,-827.1148 47,-818.2539 47,-810.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-845.7731 42.5001,-835.773 47,-840.7731 47.0001,-835.7731 47.0001,-835.7731 47.0001,-835.7731 47,-840.7731 51.5001,-835.7731 47,-845.7731 47,-845.7731\"/>\n",
       "</g>\n",
       "<!-- sequential4_dense1_fwd -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>sequential4_dense1_fwd</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"#000000\" points=\"94,-998 0,-998 0,-940 94,-940 94,-998\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-972.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-957.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">200</text>\n",
       "</g>\n",
       "<!-- sequential4_dense1_fwd&#45;&gt;sequential4_dropout1_fwd -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>sequential4_dense1_fwd&#45;&gt;sequential4_dropout1_fwd</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-929.6321C47,-921.1148 47,-912.2539 47,-904.2088\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-939.7731 42.5001,-929.773 47,-934.7731 47.0001,-929.7731 47.0001,-929.7731 47.0001,-929.7731 47,-934.7731 51.5001,-929.7731 47,-939.7731 47,-939.7731\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f178b9f8898>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.hybridize()\n",
    "net.collect_params().initialize()\n",
    "x = mx.sym.var('data')\n",
    "sym = net(x)\n",
    "mx.viz.plot_network(sym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize parameters, loss function, and training loop\n",
    "Using \"Xavier\" to initialize parameter and 'rmsprop' for the optimizer. Everything else is pretty standard with softmax_cross_entropy as the loss function. Training loop goes through 5 epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/parameter.py:811: UserWarning: Parameter 'sequential4_embedding0_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/parameter.py:811: UserWarning: Parameter 'sequential4_conv0_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/parameter.py:811: UserWarning: Parameter 'sequential4_dense0_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/parameter.py:811: UserWarning: Parameter 'sequential4_dense1_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 4.92246551154, Train_acc 0.0552458033573, Test_acc 0.0546762589928\n",
      "Epoch 1. Loss: 4.01285473884, Train_acc 0.298021582734, Test_acc 0.279616306954\n",
      "Epoch 2. Loss: 3.11311594413, Train_acc 0.475989208633, Test_acc 0.417386091127\n",
      "Epoch 3. Loss: 2.4780531084, Train_acc 0.597511990408, Test_acc 0.488729016787\n",
      "Epoch 4. Loss: 2.10568194173, Train_acc 0.644364508393, Test_acc 0.515707434053\n"
     ]
    }
   ],
   "source": [
    "# Intialize paramenters and determine loss function\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'rmsprop', {'learning_rate': 0.0005})\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]\n",
    "    \n",
    "epochs = 5\n",
    "smoothing_constant = .01\n",
    "test_accuracy_list = []\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "#         print(\"data shape is\", data.shape)\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "#             print(output.shape)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        #  Keep a moving average of the losses\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))\n",
    "#     if e == 0:\n",
    "#         filename = os.path.join('/tmp/eider-user/userfile/'+username+'/', \"testnet.params\")\n",
    "#         net.save_parameters(filename)\n",
    "#         s3.Bucket('itk-model-data').upload_file(filename, 'testnet.params')\n",
    "#     elif test_accuracy_list[e] > max(test_accuracy_list):\n",
    "#         net.save_parameters(filename)\n",
    "#         s3.Bucket('itk-model-data').upload_file(filename, 'testnet.params')\n",
    "#     else:\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top_K_acc 0.726378896882\n"
     ]
    }
   ],
   "source": [
    "# Get top K accuracy. Are the top K predicted values equal to the actual value?\n",
    "def evaluate_topk_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.TopKAccuracy(top_k=10)\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        predictions = net(data)\n",
    "        # predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]\n",
    "\n",
    "topk_accuracy = evaluate_topk_accuracy(test_data,net)\n",
    "print(\"Top_K_acc %s\" % (topk_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters\n",
    "net.save_parameters(\"testnet.params\")\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('testnet.params',bucket,'testnet.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use pickle to save all relevant training variables for use later\n",
    "with open('vocab_labels.pickle', 'wb') as f:\n",
    "    pickle.dump([vocabulary,vocab_size,sentence_size,labelencoder,unique_labels,test_accuracy_list], f)\n",
    "\n",
    "s3.upload_file('vocab_labels.pickle',bucket,'vocab_labels.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
