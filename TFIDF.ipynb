{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import brown\n",
    "import pickle\n",
    "import boto3\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Set execution role\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "resource = boto3.resource('s3') #high-level object-oriented API\n",
    "my_bucket = resource.Bucket('sagemaker-nomadiq-data') #subsitute this for your s3 bucket name. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from S3\n",
    "We download model artifacts and raw Instagram and Wikitravel data from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_bucket.download_file('instagram_df.csv','instagram_df.csv')\n",
    "my_bucket.download_file('wikitravel.csv','wikitravel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Instagram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This data contains location id and location name mappings scraped from instagram\n",
    "insta_df = pd.read_csv('instagram_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "insta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Wikitravel scrape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_df = pd.read_csv('wikitravel.csv',sep = '\\t', index_col= False)\n",
    "# Remove entries with \"errors\"\n",
    "wiki_df = wiki_df[(wiki_df['summary'] != 'error')|(wiki_df['do'] != 'error')|\n",
    "                  (wiki_df['see'] != 'error')|(wiki_df['eat'] != 'error')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Locations to Location_IDs\n",
    "Instagram IDs are often at the city + state + country level and the wiki travel data is at just the city level. We need to normalizes these to map to eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we zip the location_names and location_id from the instagram data\n",
    "zipped_locations = list(zip(insta_df.location_name,insta_df.location_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get first 5 rows of zipped data from instagram_data\n",
    "zipped_locations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We condense the large instagram scrape data by grouping to only unique combinations of location_name and location_id\n",
    "insta_df2 = insta_df.groupby(['location_name','location_id']).size().reset_index().rename(columns={0:'count'})\n",
    "zipped_locations = list(zip(insta_df2.location_name,insta_df2.location_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get first 5 rows of zipped data from instagram_data\n",
    "zipped_locations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile list of cities from wiki scrape data\n",
    "city_list = list(wiki_df.city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get first 5 rolws of city list\n",
    "city_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of city names and location ID using city list from wiki_travel and location ID mappings from instagram data.\n",
    "# Output is a key value pair of city name and location id\n",
    "city_dict = {}\n",
    "\n",
    "# Loop through all cities in the wikitravel list of cities\n",
    "for city in city_list:\n",
    "    # Inner loop through all location_name/location_id mappings\n",
    "    for location in zipped_locations:\n",
    "        # If there is an exactly city match pull write the key/value\n",
    "        # Else continue\n",
    "        if city.lower() == location[0].lower():\n",
    "            city_dict[city] = int(location[1])\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    # If no exact matches are found, we try soft matches where find the city name within the longer string in location_name\n",
    "    try:\n",
    "        city_dict[city]\n",
    "    except:\n",
    "        for location in zipped_locations:\n",
    "            if city.lower() in location[0].lower():\n",
    "                city_dict[city]= int(location[1])\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for vectorization\n",
    "The data from Wikitravel needs to be parsed and normalized prior to vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean string with preprocessing regex rules\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "#     string = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', string)\n",
    "#     string = re.sub(r\"\\d+\", \"\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"<b>\", \" \", string)\n",
    "    string = re.sub(r\"</b>\", \" \", string)\n",
    "    string = re.sub(r\"<br>\", \" \", string)\n",
    "    string = re.sub(r\"</br>\", \" \", string)\n",
    "    string = re.sub(r\"<p>\", \" \", string)\n",
    "    string = re.sub(r\"</p>\", \" \", string)\n",
    "    string = re.sub(r\"<ul>\", \" \", string)\n",
    "    string = re.sub(r\"</ul>\", \" \", string)\n",
    "    string = re.sub(r\"<li>\", \" \", string)\n",
    "    string = re.sub(r\"</li>\", \" \", string)  \n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate all text from Wikitravel text into single column for each location\n",
    "wiki_df['concat_text'] = wiki_df['summary'] + \" \" + wiki_df[\"do\"] + \" \" + wiki_df['see'] + \" \" + wiki_df['eat']\n",
    "wiki_df['concat_text'] = wiki_df['concat_text'].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit the wiki data to only those that are in the location_name/id mappings\n",
    "wiki_df = wiki_df[wiki_df['city'].isin(list(city_dict.keys()))]\n",
    "wiki_df.reset_index(drop = True, inplace = True)\n",
    "# Dedupe the dataframe\n",
    "wiki_df = wiki_df.drop_duplicates(['city'])\n",
    "print(\"Number of Cities: \", len(wiki_df))\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary and stopword list\n",
    "The problem with using TFIDF for travel data is that location specific data will be heavily weighted. For example, an article about Tokyo will have the word \"Toyko\" appear many times. Tokyo is likely not included in many other articles except for others that are about Japan (e.g, Osaka, Kyoto, etc.). This would make the recommendation engine overly specific. Therefore, we limit the vocabulary to the english dictionary and remove location-specific strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create unique location list and city list to include into stopwords\n",
    "location_name_list = list(insta_df2.location_name.unique().astype(str))\n",
    "location_name_list = [x.lower() for x in location_name_list]\n",
    "cities = pd.read_csv('cities.csv', index_col= False)\n",
    "city_list = list(cities['city'].astype(str)) + list(cities['country'].astype(str))\n",
    "city_list = [x.lower() for x in city_list]\n",
    "location_list = set(location_name_list + city_list)\n",
    "location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_list2 = []\n",
    "for city in city_list:\n",
    "    split_city = city.split()\n",
    "    for split in split_city:\n",
    "        city_list2.append(split)\n",
    "\n",
    "location_list = set(location_name_list + city_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate NLTK words and brown corpuses for comprehensive vocabulary\n",
    "english_vocab = set([w.lower() for w in words.words()] + [w.lower() for w in brown.words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Custom Stopwords unioning all english words and location-specific strings\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(location_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize text using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We instantiate the TfidifVectorizer using our customer stopwords and limiting to the english vocab\n",
    "vectorizer = TfidfVectorizer(stop_words = my_stop_words, vocabulary = english_vocab)\n",
    "X_tfidf = vectorizer.fit_transform(wiki_df['concat_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of Features:\", len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get number of features\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Cosine Similarity\n",
    "We use cosine similarity to determine pairwise similiarities of locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We pass the TFIDF matrix to get pairwise similarities of all entries. \n",
    "cosine_sim = linear_kernel(X_tfidf, X_tfidf)\n",
    "# example output of cosine similarity for our first city \"A Coruna\"\n",
    "# There is a score for every city from 0 to 1 (1 means that the pair is identical).\n",
    "cosine_sim[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find most similar cities to seed city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct a reverse map of cities to indices\n",
    "indices = pd.Series(wiki_df.city)\n",
    "indices.reset_index()\n",
    "reverse_indices = pd.Series(wiki_df.index, index=wiki_df['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle similarity score data, indices, wiki_df, tfidf_vectorizer, x_tfidf\n",
    "with open('tfidf_artifacts.pickle', 'wb') as f:\n",
    "    pickle.dump([cosine_sim,indices,reverse_indices,vectorizer,X_tfidf,city_dict], f)\n",
    "\n",
    "my_bucket.upload_file('tfidf_artifacts.pickle','tfidf_artifacts.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that takes in city as input and outputs most similar cities\n",
    "# Pickle sim_scores, indices, wiki_df, tfidf vectorizer, x_tfidf, clean_str?\n",
    "\n",
    "def get_recommendations_city(city, cosine_sim=cosine_sim):\n",
    "    city = city.title()\n",
    "    idx = reverse_indices[city]\n",
    "    # Get the pairwise similarity scores of all cities\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the cities based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores of the 10 most similar cities\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the city indices\n",
    "    city_indices = [i[0] for i in sim_scores]\n",
    "    city_recs = []\n",
    "    loop_count = 0\n",
    "    for city in indices.iloc[city_indices]:\n",
    "        city_recs.append({\"location_id\": city_dict[city], \"location_name\": city, \"cosine_similarity\": sim_scores[loop_count][1]})\n",
    "        loop_count += 1\n",
    "    # Return the top 10 most similar cities\n",
    "    return json.dumps(city_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"location_id\": 282991098, \"location_name\": \"Toyama\", \"cosine_similarity\": 0.33108362726572177}, {\"location_id\": 246003568, \"location_name\": \"Kusatsu\", \"cosine_similarity\": 0.26432142461217406}, {\"location_id\": 388032555, \"location_name\": \"Kitakyushu\", \"cosine_similarity\": 0.26155100986898505}, {\"location_id\": 216289572, \"location_name\": \"Saitama\", \"cosine_similarity\": 0.24143604747506808}, {\"location_id\": 363044171, \"location_name\": \"Busan\", \"cosine_similarity\": 0.24004022868819272}, {\"location_id\": 243652040, \"location_name\": \"Sasebo\", \"cosine_similarity\": 0.23542860728034753}, {\"location_id\": 241120524, \"location_name\": \"Hachinohe\", \"cosine_similarity\": 0.23097162848893407}, {\"location_id\": 235447538, \"location_name\": \"Sapporo\", \"cosine_similarity\": 0.22906304067391015}, {\"location_id\": 234915980, \"location_name\": \"Machida\", \"cosine_similarity\": 0.2288593980652679}, {\"location_id\": 579940741, \"location_name\": \"Sendai\", \"cosine_similarity\": 0.228657650727346}]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter city name to get top 10 city names\n",
    "get_recommendations_city('Seattle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find cities most similar to keyword input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recommendations_keywords(doc, cosine_sim=cosine_sim):\n",
    "    test_tfidf = vectorizer.transform([clean_str(doc)])\n",
    "    cosine_sim_test = linear_kernel(test_tfidf, X_tfidf)\n",
    "    sim_scores = list(enumerate(cosine_sim_test[0]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[0:10]\n",
    "    city_indices = [i[0] for i in sim_scores]\n",
    "    city_recs = []\n",
    "    loop_count = 0\n",
    "    for city in wiki_df['city'].iloc[city_indices]:\n",
    "        city_recs.append({\"location_id\": city_dict[city], \"location_name\": city, \"cosine_similarity\": sim_scores[loop_count][1]})\n",
    "        loop_count += 1\n",
    "    # Return the top 10 most similar cities\n",
    "    return json.dumps(city_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get similar locations to a given keyword vector\n",
    "get_recommendations_keywords(\"scuba dive seafood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you only needs to do this once, this is a mapping of index to \n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# get the document that we want to \n",
    "# doc = df.concat_text[2101]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector = vectorizer.transform([doc])\n",
    " \n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(features,sorted_items,10)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r'tfidf_artifacts.pickle', 'rb') as f:\n",
    "    cosine_sim,indices,reverse_indices,vectorizer,X_tfidf,city_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x261552 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 132 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
